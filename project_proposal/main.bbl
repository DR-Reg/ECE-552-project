% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{HSA-paper}
\BIBentryALTinterwordspacing
C.-T. Chen, H.~Mun, J.~Meng, M.~S. Abdelfattah, and J.~sun Seo, ``Hybrid systolic array accelerator with optimized dataflow for edge large language model inference,'' 2025. [Online]. Available: \url{https://arxiv.org/abs/2507.09010}
\BIBentrySTDinterwordspacing

\bibitem{sparse-tpu}
\BIBentryALTinterwordspacing
A.~A. S. F. D.-H. P. A. R. H. Y. Y. C. R. D. T.~M. Xin~He, Subhankar~Pal, ``Sparse-tpu: Adapting systolic arrays for sparse matrices,'' 2020. [Online]. Available: \url{https://tnm.engin.umich.edu/wp-content/uploads/sites/353/2020/08/2020.6.sparse-tpu_ics2020.pdfhttps://arxiv.org/abs/2507.09010}
\BIBentrySTDinterwordspacing

\bibitem{LLM-demand-growth}
\BIBentryALTinterwordspacing
A.~Fradkin, ``Demand for llms: Descriptive evidence on substitution, market expansion, and multihoming,'' 2025. [Online]. Available: \url{https://arxiv.org/abs/2504.15440}
\BIBentrySTDinterwordspacing

\bibitem{sparse-llm}
\BIBentryALTinterwordspacing
G.~Bai, Y.~Li, C.~Ling, K.~Kim, and L.~Zhao, ``Sparsellm: Towards global pruning for pre-trained language models,'' 2024. [Online]. Available: \url{https://arxiv.org/abs/2402.17946}
\BIBentrySTDinterwordspacing

\bibitem{sparse-gpt}
\BIBentryALTinterwordspacing
E.~Frantar and D.~Alistarh, ``Sparsegpt: Massive language models can be accurately pruned in one-shot,'' 2023. [Online]. Available: \url{https://arxiv.org/abs/2301.00774}
\BIBentrySTDinterwordspacing

\bibitem{KMZ}
\BIBentryALTinterwordspacing
H.~Kung, B.~McDanel, and S.~Q. Zhang, ``Packing sparse convolutional neural networks for efficient systolic array implementations: Column combining under joint optimization,'' in \emph{Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems}, ser. ASPLOS '19.\hskip 1em plus 0.5em minus 0.4em\relax New York, NY, USA: Association for Computing Machinery, 2019, p. 821–834. [Online]. Available: \url{https://doi.org/10.1145/3297858.3304028}
\BIBentrySTDinterwordspacing

\bibitem{cpu-outperform-on-device-llm-inference}
\BIBentryALTinterwordspacing
H.~Zhang and J.~Huang, ``Challenging gpu dominance: When cpus outperform for on-device llm inference,'' 2025. [Online]. Available: \url{https://arxiv.org/abs/2505.06461}
\BIBentrySTDinterwordspacing

\bibitem{enhancing-llm-inference-on-arm-cpu}
C.~Zhang, X.~Zhu, L.~Chen, T.~Yang, E.~Pan, G.~Yu, Y.~Zhao, X.~Wu, B.~Li, W.~Mao, and G.~Han, ``Enhancing llm inference performance on arm cpus through software and hardware co-optimization strategies,'' \emph{Integrated Circuits and Systems}, vol.~2, no.~2, pp. 49--57, 2025.

\bibitem{efficient-llm-inference-on-cpu}
\BIBentryALTinterwordspacing
H.~Shen, H.~Chang, B.~Dong, Y.~Luo, and H.~Meng, ``Efficient llm inference on cpus,'' 2023. [Online]. Available: \url{https://arxiv.org/abs/2311.00502}
\BIBentrySTDinterwordspacing

\bibitem{advances-low-bit-quant-llm-edge}
\BIBentryALTinterwordspacing
S.~Cao, L.~Ma, and T.~Cao, ``Advances to low-bit quantization enable llms on edge devices,'' 2025. [Online]. Available: \url{https://arxiv.org/abs/2311.00502}
\BIBentrySTDinterwordspacing

\bibitem{how-to-run-large-ai-model-edge-device}
\BIBentryALTinterwordspacing
K.~Freund, ``How to run large ai models on an edge device,'' 2023. [Online]. Available: \url{https://cambrian-ai.com/how-to-run-large-ai-models-on-an-edge-device/}
\BIBentrySTDinterwordspacing

\bibitem{llama.cpp}
\BIBentryALTinterwordspacing
ggml org, ``llama.cpp,'' 2025. [Online]. Available: \url{https://github.com/ggml-org/llama.cpp}
\BIBentrySTDinterwordspacing

\bibitem{media-pipe}
\BIBentryALTinterwordspacing
C.~Lugaresi, J.~Tang, H.~Nash, C.~McClanahan, E.~Uboweja, M.~Hays, F.~Zhang, C.-L. Chang, M.~G. Yong, J.~Lee, W.-T. Chang, W.~Hua, M.~Georg, and M.~Grundmann, ``Mediapipe: A framework for building perception pipelines,'' 2019. [Online]. Available: \url{https://arxiv.org/abs/1906.08172}
\BIBentrySTDinterwordspacing

\bibitem{google-seminal-paper}
\BIBentryALTinterwordspacing
N.~P. Jouppi, C.~Young, N.~Patil, D.~Patterson, G.~Agrawal, R.~Bajwa, S.~Bates, S.~Bhatia, N.~Boden, A.~Borchers, R.~Boyle, P.~luc Cantin, C.~Chao, C.~Clark, J.~Coriell, M.~Daley, M.~Dau, J.~Dean, B.~Gelb, T.~V. Ghaemmaghami, R.~Gottipati, W.~Gulland, R.~Hagmann, C.~R. Ho, D.~Hogberg, J.~Hu, R.~Hundt, D.~Hurt, J.~Ibarz, A.~Jaffey, A.~Jaworski, A.~Kaplan, H.~Khaitan, A.~Koch, N.~Kumar, S.~Lacy, J.~Laudon, J.~Law, D.~Le, C.~Leary, Z.~Liu, K.~Lucke, A.~Lundin, G.~MacKean, A.~Maggiore, M.~Mahony, K.~Miller, R.~Nagarajan, R.~Narayanaswami, R.~Ni, K.~Nix, T.~Norrie, M.~Omernick, N.~Penukonda, A.~Phelps, J.~Ross, M.~Ross, A.~Salek, E.~Samadiani, C.~Severn, G.~Sizikov, M.~Snelham, J.~Souter, D.~Steinberg, A.~Swing, M.~Tan, G.~Thorson, B.~Tian, H.~Toma, E.~Tuttle, V.~Vasudevan, R.~Walter, W.~Wang, E.~Wilcox, and D.~H. Yoon, ``In-datacenter performance analysis of a tensor processing unit,'' 2017. [Online]. Available: \url{https://arxiv.org/abs/1704.04760}
\BIBentrySTDinterwordspacing

\bibitem{heterogeneous}
\BIBentryALTinterwordspacing
L.~Chen, D.~Feng, E.~Feng, Y.~Wang, R.~Zhao, Y.~Xia, P.~Xu, and H.~Chen, ``Characterizing mobile soc for accelerating heterogeneous llm inference,'' 2025. [Online]. Available: \url{https://arxiv.org/abs/2501.14794}
\BIBentrySTDinterwordspacing

\bibitem{NPU-PIM}
\BIBentryALTinterwordspacing
M.~Seo, X.~T. Nguyen, S.~J. Hwang, Y.~Kwon, G.~Kim, C.~Park, I.~Kim, J.~Park, J.~Kim, W.~Shin, J.~Won, H.~Choi, K.~Kim, D.~Kwon, C.~Jeong, S.~Lee, Y.~Choi, W.~Byun, S.~Baek, H.-J. Lee, and J.~Kim, ``Ianus: Integrated accelerator based on npu-pim unified memory system,'' in \emph{Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3}, ser. ASPLOS '24.\hskip 1em plus 0.5em minus 0.4em\relax New York, NY, USA: Association for Computing Machinery, 2024, p. 545–560. [Online]. Available: \url{https://doi.org/10.1145/3620666.3651324}
\BIBentrySTDinterwordspacing

\bibitem{kv-cache-compressed}
\BIBentryALTinterwordspacing
A.~Liu, J.~Liu, Z.~Pan, Y.~He, G.~Haffari, and B.~Zhuang, ``Minicache: Kv cache compression in depth dimension for large language models,'' 2024. [Online]. Available: \url{https://arxiv.org/abs/2405.14366}
\BIBentrySTDinterwordspacing

\bibitem{kv-cache-reuse}
\BIBentryALTinterwordspacing
J.~Thomson, A.~Shah, and L.~Tewari, ``Introducing new kv cache reuse optimizations in nvidia tensorrt-llm,'' 2025. [Online]. Available: \url{https://developer.nvidia.com/blog/introducing-new-kv-cache-reuse-optimizations-in-nvidia-tensorrt-llm/}
\BIBentrySTDinterwordspacing

\bibitem{sys-verilog}
``Ieee standard for systemverilog--unified hardware design, specification, and verification language,'' \emph{IEEE Std 1800-2023 (Revision of IEEE Std 1800-2017)}, pp. 1--1354, 2024.

\end{thebibliography}
