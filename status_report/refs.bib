@misc{LLM-demand-growth,
      title={Demand for LLMs: Descriptive Evidence on Substitution, Market Expansion, and Multihoming}, 
      author={Andrey Fradkin},
      year={2025},
      eprint={2504.15440},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2504.15440}, 
}
@misc{HSA-paper,
      title={Hybrid Systolic Array Accelerator with Optimized Dataflow for Edge Large Language Model Inference}, 
      author={Chun-Ting Chen and HanGyeol Mun and Jian Meng and Mohamed S. Abdelfattah and Jae-sun Seo},
      year={2025},
      eprint={2507.09010},
      archivePrefix={arXiv},
      primaryClass={cs.AR},
      url={https://arxiv.org/abs/2507.09010}, 
}
@misc{sparse-tpu,
      title={Sparse-TPU: Adapting Systolic Arrays for Sparse Matrices},
      author={Xin He, Subhankar Pal, Aporva Amarnath, Siying Feng, Dong-Hyeon Park, Austin Rovinski, Haojie Ye, Yuhan Chen, Ronald Dreslinski, Trevor Mudge},
      year={2020},
      url={https://tnm.engin.umich.edu/wp-content/uploads/sites/353/2020/08/2020.6.sparse-tpu_ics2020.pdfhttps://arxiv.org/abs/2507.09010}, 
}
@misc{sparse-gpt,
	      title={SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot}, 
	            author={Elias Frantar and Dan Alistarh},
		          year={2023},
			        eprint={2301.00774},
				            primaryClass={cs.LG},
					          url={https://arxiv.org/abs/2301.00774}, 
}
@misc{sparse-llm,
      title={SparseLLM: Towards Global Pruning for Pre-trained Language Models}, 
      author={Guangji Bai and Yijiang Li and Chen Ling and Kibaek Kim and Liang Zhao},
      year={2024},
      eprint={2402.17946},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.17946}, 
}
@inproceedings{KMZ,
	author = {Kung, H.T. and McDanel, Bradley and Zhang, Sai Qian},
	title = {Packing Sparse Convolutional Neural Networks for Efficient Systolic Array Implementations: Column Combining Under Joint Optimization},
	year = {2019},
	isbn = {9781450362405},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3297858.3304028},
	doi = {10.1145/3297858.3304028},
	abstract = {This paper describes a novel approach of packing sparse convolutional neural networks into a denser format for efficient implementations using systolic arrays. By combining multiple sparse columns of a convolutional filter matrix into a single dense column stored in the systolic array, the utilization efficiency of the systolic array can be substantially increased (e.g., 8x) due to the increased density of nonzero weights in the resulting packed filter matrix. In combining columns, for each row, all filter weights but the one with the largest magnitude are pruned. The remaining weights are retrained to preserve high accuracy. We study the effectiveness of this joint optimization for both high utilization efficiency and classification accuracy with ASIC and FPGA designs based on efficient bit-serial implementations of multiplier-accumulators. We demonstrate that in mitigating data privacy concerns the retraining can be accomplished with only fractions of the original dataset (e.g., 10\% for CIFAR-10). We present analysis and empirical evidence on the superior performance of our column combining approach against prior arts under metrics such as energy efficiency (3x) and inference latency (12x).},
	booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
	pages = {821–834},
	numpages = {14},
	keywords = {data flow architectures, joint optimization, neural networks, sparsity, systolic arrays},
	location = {Providence, RI, USA},
	series = {ASPLOS '19}
}
@misc{cpu-outperform-on-device-llm-inference,
	      title={Challenging GPU Dominance: When CPUs Outperform for On-Device LLM Inference}, 
	            author={Haolin Zhang and Jeff Huang},
		          year={2025},
			        eprint={2505.06461},
				      archivePrefix={arXiv},
				            primaryClass={cs.DC},
					          url={https://arxiv.org/abs/2505.06461}, 
}
@ARTICLE{enhancing-llm-inference-on-arm-cpu,
  author={Zhang, Cheng and Zhu, Xingyu and Chen, Longhao and Yang, Tingjie and Pan, Evens and Yu, Guosheng and Zhao, Yang and Wu, Xiguang and Li, Bo and Mao, Wei and Han, Genquan},
  journal={Integrated Circuits and Systems}, 
  title={Enhancing LLM Inference Performance on ARM CPUs Through Software and Hardware Co-Optimization Strategies}, 
  year={2025},
  volume={2},
  number={2},
  pages={49-57},
  keywords={Quantization (signal);Computational modeling;Hardware;Software;Accuracy;Single instruction multiple data;Registers;Throughput;Reduced instruction set computing;Performance evaluation;Model compression;mixed-precision quantization;ARM CPUs;SIMD optimization;LLM inference performance},
  doi={10.23919/ICS.2025.3568404}}

@misc{efficient-llm-inference-on-cpu,
      title={Efficient LLM Inference on CPUs}, 
      author={Haihao Shen and Hanwen Chang and Bo Dong and Yu Luo and Hengyu Meng},
      year={2023},
      eprint={2311.00502},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.00502}, 
}

@misc{advances-low-bit-quant-llm-edge,
      title={Advances to low-bit quantization enable LLMs on edge devices}, 
      author={Shijie Cao and Lingxiao Ma and Ting Cao},
      year={2025},
      url={https://arxiv.org/abs/2311.00502},
}

@misc{how-to-run-large-ai-model-edge-device,
	title={How To Run Large AI Models On An Edge Device},
	author={Karl Freund},
	year={2023},
	url={https://cambrian-ai.com/how-to-run-large-ai-models-on-an-edge-device/}
}
@misc{llama.cpp,
	title={llama.cpp},
	author={ggml-org},
	year={2025},
	url={https://github.com/ggml-org/llama.cpp}
}
@misc{media-pipe,
	      title={MediaPipe: A Framework for Building Perception Pipelines}, 
	            author={Camillo Lugaresi and Jiuqiang Tang and Hadon Nash and Chris McClanahan and Esha Uboweja and Michael Hays and Fan Zhang and Chuo-Ling Chang and Ming Guang Yong and Juhyun Lee and Wan-Teh Chang and Wei Hua and Manfred Georg and Matthias Grundmann},
		          year={2019},
			        eprint={1906.08172},
				      archivePrefix={arXiv},
				            primaryClass={cs.DC},
					          url={https://arxiv.org/abs/1906.08172}, 
}
@misc{heterogeneous,
	      title={Characterizing Mobile SoC for Accelerating Heterogeneous LLM Inference}, 
	            author={Le Chen and Dahu Feng and Erhu Feng and Yingrui Wang and Rong Zhao and Yubin Xia and Pinjie Xu and Haibo Chen},
		          year={2025},
			        eprint={2501.14794},
				      archivePrefix={arXiv},
				            primaryClass={cs.DC},
					          url={https://arxiv.org/abs/2501.14794}, 
}

@inproceedings{NPU-PIM,
author = {Seo, Minseok and Nguyen, Xuan Truong and Hwang, Seok Joong and Kwon, Yongkee and Kim, Guhyun and Park, Chanwook and Kim, Ilkon and Park, Jaehan and Kim, Jeongbin and Shin, Woojae and Won, Jongsoon and Choi, Haerang and Kim, Kyuyoung and Kwon, Daehan and Jeong, Chunseok and Lee, Sangheon and Choi, Yongseok and Byun, Wooseok and Baek, Seungcheol and Lee, Hyuk-Jae and Kim, John},
title = {IANUS: Integrated Accelerator based on NPU-PIM Unified Memory System},
year = {2024},
isbn = {9798400703867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620666.3651324},
doi = {10.1145/3620666.3651324},
abstract = {Accelerating end-to-end inference of transformer-based large language models (LLMs) is a critical component of AI services in datacenters. However, the diverse compute characteristics of LLMs' end-to-end inference present challenges as previously proposed accelerators only address certain operations or stages (e.g., self-attention, generation stage, etc.). To address the unique challenges of accelerating end-to-end inference, we propose IANUS - Integrated Accelerator based on NPU-PIM Unified Memory System. IANUS is a domain-specific system architecture that combines a Neural Processing Unit (NPU) with a Processing-in-Memory (PIM) to leverage both the NPU's high computation throughput and the PIM's high effective memory bandwidth. In particular, IANUS employs a unified main memory system where the PIM memory is used both for PIM operations and for NPU's main memory. The unified main memory system ensures that memory capacity is efficiently utilized and the movement of shared data between NPU and PIM is minimized. However, it introduces new challenges since normal memory accesses and PIM computations cannot be performed simultaneously. Thus, we propose novel PIM Access Scheduling that manages not only the scheduling of normal memory accesses and PIM computations but also workload mapping across the PIM and the NPU. Our detailed simulation evaluations show that IANUS improves the performance of GPT-2 by 6.2\texttimes{} and 3.2\texttimes{}, on average, compared to the NVIDIA A100 GPU and the state-of-the-art accelerator. As a proof-of-concept, we develop a prototype of IANUS with a commercial PIM, NPU, and an FPGA-based PIM controller to demonstrate the feasibility of IANUS.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {545–560},
numpages = {16},
keywords = {accelerators, heterogeneous architectures, neural processing unit, processing-in-memory, large language model, workload mapping, scheduling},
location = {La Jolla, CA, USA},
series = {ASPLOS '24}
}

@misc{kv-cache-compressed,
	      title={MiniCache: KV Cache Compression in Depth Dimension for Large Language Models}, 
	            author={Akide Liu and Jing Liu and Zizheng Pan and Yefei He and Gholamreza Haffari and Bohan Zhuang},
		          year={2024},
			        eprint={2405.14366},
				      archivePrefix={arXiv},
				            primaryClass={cs.CL},
					          url={https://arxiv.org/abs/2405.14366}, 
}
@misc{kv-cache-reuse,
	title={Introducing New KV Cache Reuse Optimizations in NVIDIA TensorRT-LLM},
	author={John Thomson and Anjali Shah and Laikh Tewari},
	year={2025},
	url={https://developer.nvidia.com/blog/introducing-new-kv-cache-reuse-optimizations-in-nvidia-tensorrt-llm/}
}
@misc{google-seminal-paper,
	      title={In-Datacenter Performance Analysis of a Tensor Processing Unit}, 
	            author={Norman P. Jouppi and Cliff Young and Nishant Patil and David Patterson and Gaurav Agrawal and Raminder Bajwa and Sarah Bates and Suresh Bhatia and Nan Boden and Al Borchers and Rick Boyle and Pierre-luc Cantin and Clifford Chao and Chris Clark and Jeremy Coriell and Mike Daley and Matt Dau and Jeffrey Dean and Ben Gelb and Tara Vazir Ghaemmaghami and Rajendra Gottipati and William Gulland and Robert Hagmann and C. Richard Ho and Doug Hogberg and John Hu and Robert Hundt and Dan Hurt and Julian Ibarz and Aaron Jaffey and Alek Jaworski and Alexander Kaplan and Harshit Khaitan and Andy Koch and Naveen Kumar and Steve Lacy and James Laudon and James Law and Diemthu Le and Chris Leary and Zhuyuan Liu and Kyle Lucke and Alan Lundin and Gordon MacKean and Adriana Maggiore and Maire Mahony and Kieran Miller and Rahul Nagarajan and Ravi Narayanaswami and Ray Ni and Kathy Nix and Thomas Norrie and Mark Omernick and Narayana Penukonda and Andy Phelps and Jonathan Ross and Matt Ross and Amir Salek and Emad Samadiani and Chris Severn and Gregory Sizikov and Matthew Snelham and Jed Souter and Dan Steinberg and Andy Swing and Mercedes Tan and Gregory Thorson and Bo Tian and Horia Toma and Erick Tuttle and Vijay Vasudevan and Richard Walter and Walter Wang and Eric Wilcox and Doe Hyun Yoon},
		          year={2017},
			        eprint={1704.04760},
				      archivePrefix={arXiv},
				            primaryClass={cs.AR},
					          url={https://arxiv.org/abs/1704.04760}, 
}

@ARTICLE{sys-verilog,
  author={},
  journal={IEEE Std 1800-2023 (Revision of IEEE Std 1800-2017)}, 
  title={IEEE Standard for SystemVerilog--Unified Hardware Design, Specification, and Verification Language}, 
  year={2024},
  volume={},
  number={},
  pages={1-1354},
  keywords={IEEE Standards;Design automation;Hardware design languages;Formal verification;Computer languages;System analysis and design;assertions;design automation;design verification;hardware description language;HDL;HDVL;IEEE Std 1800™;PLI;programming language interface;SystemVerilog;Verilog®;VPI},
  doi={10.1109/IEEESTD.2024.10458102}}
